{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for training VAE on Tabula Muris heart data\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as ds\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "from nets import *\n",
    "from lib import *\n",
    "\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, num_cells_train, gex_size):\n",
    "        self.num_cells_train =  num_cells_train  # number of cells\n",
    "        self.gex_size = gex_size                 # number of genes\n",
    "        self.epsilon_use = 1e-16                 # small constant value\n",
    "        self.n_train_epochs = 100                 # number of epochs for training\n",
    "        self.batch_size = 32                     # batch size of the GAN-based methods\n",
    "        self.vae_batch_size = 128                # batch size of the VAE-based methods\n",
    "        self.code_size = 10                      # number of codes\n",
    "        self.noise_size = 118                    # number of noise variables\n",
    "        self.inflate_to_size1 = 256              # number of neurons\n",
    "        self.inflate_to_size2 = 512              # number of neurons\n",
    "        self.inflate_to_size3 = 1024             # number of neurons\n",
    "        self.TotalCorrelation_lamb = 0.0         # hyperparameter for the total correlation penalty in beta-TCVAE\n",
    "        self.InfoGAN_fix_std = True              # fixing the standard deviation or not for the Q network of InfoGAN\n",
    "        self.dropout_rate = 0.2                  # dropout hyperparameter\n",
    "        self.disc_internal_size1 = 1024          # number of neurons\n",
    "        self.disc_internal_size2 = 512           # number of neurons\n",
    "        self.disc_internal_size3 = 10            # number of neurons\n",
    "        self.num_cells_generate = 3000           # number of sampled cells\n",
    "        self.GradientPenaly_lambda = 10.0        # hyperparameter for the gradient penalty of Wasserstein GANs\n",
    "        self.latentSample_size = 1               # number of samples of the encoder of VAEs\n",
    "        self.MutualInformation_lamb = 10.0       # hyperparameter for the mutual information penalty in InfoGAN\n",
    "        self.Diters = 5                          # number of training discriminator network per training of generator network of Wasserstein GANs\n",
    "        self.model_path = \"./examples/models_vae/\"        # path saving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load data and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.load('./data/TabulaMurisHeart_Processed.npy')\n",
    "data_meta = pd.read_csv(\"./data/TabulaMurisHeart_MetaInformation.csv\")\n",
    "opt = Options(data_matrix.shape[0], data_matrix.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Define network tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_v = tf.placeholder(tf.float32, shape = (None, opt.code_size))\n",
    "X_v = tf.placeholder(tf.float32, shape = (None, opt.gex_size))\n",
    "\n",
    "## encoder\n",
    "z_gen_mean_v, z_gen_std_v = vaes_encoder(X_v, opt)\n",
    "\n",
    "### reparameterization of latent space\n",
    "batch_size = tf.shape(z_gen_mean_v)[0]\n",
    "eps = tf.random_normal(shape=[batch_size, opt.code_size])\n",
    "z_gen_data_v = z_gen_mean_v + z_gen_std_v * eps\n",
    "\n",
    "### latent entropies in a minibatch\n",
    "margin_entropy_mss, joint_entropy_mss = estimate_minibatch_mss_entropy(z_gen_mean_v, z_gen_std_v, z_gen_data_v, opt)\n",
    "### total correlation in a minibatch\n",
    "TotalCorre_mss = tf.reduce_sum(margin_entropy_mss) - tf.reduce_sum(joint_entropy_mss)\n",
    "\n",
    "## decoder\n",
    "z_gen_decoder = vaes_decoder(z_gen_data_v, opt)\n",
    "\n",
    "### generated data\n",
    "X_gen_data = z_gen_decoder.sample(opt.latentSample_size)\n",
    "X_gen_data = tf.reshape(X_gen_data , tf.shape(X_gen_data)[1:])\n",
    "\n",
    "## loss elements\n",
    "### reconstruction error\n",
    "z_gen_de = z_gen_decoder.log_prob(X_v)\n",
    "z_gen_de_value = tf.reduce_sum(z_gen_de, [1])\n",
    "rec_x_loss = - tf.reduce_mean(z_gen_de_value)\n",
    "\n",
    "### latent prior and posterior probabilities\n",
    "stg_prior = tf_standardGaussian_prior(tf.shape(X_v)[0], opt.code_size)\n",
    "latent_prior = stg_prior.log_prob(z_gen_data_v)\n",
    "latent_posterior = c_mutual_mu_var_entropy(z_gen_mean_v, z_gen_std_v, z_gen_data_v, opt)\n",
    "\n",
    "### latent joint prior and posterior probabilities\n",
    "latent_prior_joint = tf.reduce_sum(latent_prior, [1])\n",
    "latent_posterior_joint = tf.reduce_sum(latent_posterior, [1])\n",
    "\n",
    "### KL divergence\n",
    "kl_latent = - tf.reduce_mean(latent_prior_joint) + tf.reduce_mean(latent_posterior_joint)\n",
    "\n",
    "### VAE/beta-TCVAE loss function\n",
    "obj_vae = rec_x_loss  + kl_latent + opt.TotalCorrelation_lamb * TotalCorre_mss\n",
    "\n",
    "## time step\n",
    "time_step = tf.placeholder(tf.int32)\n",
    "\n",
    "## training tensors \n",
    "tf_all_vars = tf.trainable_variables()\n",
    "encodervar  = [var for var in tf_all_vars if var.name.startswith(\"EncoderX2Z\")]\n",
    "decodervar  = [var for var in tf_all_vars if var.name.startswith(\"DecoderZ2X\")]\n",
    "\n",
    "optimizer_vae = tf.train.AdamOptimizer(1e-4)\n",
    "opt_vae = optimizer_vae.minimize(obj_vae, var_list = encodervar + decodervar)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False, dtype = tf.int32)\n",
    "\n",
    "sess = tf.InteractiveSession()\t\n",
    "init = tf.global_variables_initializer().run()\n",
    "assign_step_zero = tf.assign(global_step, 0)\n",
    "init_step = sess.run(assign_step_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Training the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; iteration: 32; VAE loss:4514.8427734375; Total Correlation:0.15132808685302734\n",
      "epoch: 1; iteration: 64; VAE loss:4496.0986328125; Total Correlation:1.340597152709961\n",
      "epoch: 2; iteration: 96; VAE loss:4472.32373046875; Total Correlation:0.8661832809448242\n",
      "epoch: 3; iteration: 128; VAE loss:4433.85205078125; Total Correlation:1.2700738906860352\n",
      "epoch: 4; iteration: 160; VAE loss:4514.21826171875; Total Correlation:1.6838579177856445\n",
      "epoch: 5; iteration: 192; VAE loss:4515.64501953125; Total Correlation:2.9556503295898438\n",
      "epoch: 6; iteration: 224; VAE loss:4512.99365234375; Total Correlation:3.426255226135254\n",
      "epoch: 7; iteration: 256; VAE loss:4461.0419921875; Total Correlation:2.927652359008789\n",
      "epoch: 8; iteration: 288; VAE loss:4380.36865234375; Total Correlation:2.786497116088867\n",
      "epoch: 9; iteration: 320; VAE loss:4464.46875; Total Correlation:3.6780948638916016\n",
      "epoch: 10; iteration: 352; VAE loss:4454.73388671875; Total Correlation:3.2763500213623047\n",
      "epoch: 11; iteration: 384; VAE loss:4453.6416015625; Total Correlation:3.5691823959350586\n",
      "epoch: 12; iteration: 416; VAE loss:4462.52294921875; Total Correlation:3.127455711364746\n",
      "epoch: 13; iteration: 448; VAE loss:4438.25; Total Correlation:3.770111083984375\n",
      "epoch: 14; iteration: 480; VAE loss:4444.6650390625; Total Correlation:3.635530471801758\n",
      "epoch: 15; iteration: 512; VAE loss:4444.0595703125; Total Correlation:3.86862850189209\n",
      "epoch: 16; iteration: 544; VAE loss:4477.0224609375; Total Correlation:3.9563369750976562\n",
      "epoch: 17; iteration: 576; VAE loss:4370.96826171875; Total Correlation:3.0482797622680664\n",
      "epoch: 18; iteration: 608; VAE loss:4416.9951171875; Total Correlation:2.7989110946655273\n",
      "epoch: 19; iteration: 640; VAE loss:4369.1279296875; Total Correlation:3.6983509063720703\n",
      "epoch: 20; iteration: 672; VAE loss:4426.275390625; Total Correlation:3.7401323318481445\n",
      "epoch: 21; iteration: 704; VAE loss:4430.50048828125; Total Correlation:3.611278533935547\n",
      "epoch: 22; iteration: 736; VAE loss:4448.95556640625; Total Correlation:3.5818662643432617\n",
      "epoch: 23; iteration: 768; VAE loss:4399.83544921875; Total Correlation:3.616971015930176\n",
      "epoch: 24; iteration: 800; VAE loss:4356.81103515625; Total Correlation:3.6073551177978516\n",
      "epoch: 25; iteration: 832; VAE loss:4441.322265625; Total Correlation:4.02857780456543\n",
      "epoch: 26; iteration: 864; VAE loss:4442.013671875; Total Correlation:3.619441032409668\n",
      "epoch: 27; iteration: 896; VAE loss:4485.3994140625; Total Correlation:4.860288619995117\n",
      "epoch: 28; iteration: 928; VAE loss:4435.71630859375; Total Correlation:4.2449235916137695\n",
      "epoch: 29; iteration: 960; VAE loss:4375.421875; Total Correlation:3.7386064529418945\n",
      "epoch: 30; iteration: 992; VAE loss:4378.60302734375; Total Correlation:3.6065597534179688\n",
      "epoch: 31; iteration: 1024; VAE loss:4422.6611328125; Total Correlation:4.223006248474121\n",
      "epoch: 32; iteration: 1056; VAE loss:4383.8017578125; Total Correlation:3.206544876098633\n",
      "epoch: 33; iteration: 1088; VAE loss:4444.126953125; Total Correlation:4.5416669845581055\n",
      "epoch: 34; iteration: 1120; VAE loss:4404.63623046875; Total Correlation:4.251176834106445\n",
      "epoch: 35; iteration: 1152; VAE loss:4398.40576171875; Total Correlation:3.918972969055176\n",
      "epoch: 36; iteration: 1184; VAE loss:4429.609375; Total Correlation:3.95656681060791\n",
      "epoch: 37; iteration: 1216; VAE loss:4387.8994140625; Total Correlation:3.6566200256347656\n",
      "epoch: 38; iteration: 1248; VAE loss:4377.86083984375; Total Correlation:3.4601755142211914\n",
      "epoch: 39; iteration: 1280; VAE loss:4365.25341796875; Total Correlation:3.0100936889648438\n",
      "epoch: 40; iteration: 1312; VAE loss:4415.2548828125; Total Correlation:4.2042999267578125\n",
      "epoch: 41; iteration: 1344; VAE loss:4411.64111328125; Total Correlation:3.9481983184814453\n",
      "epoch: 42; iteration: 1376; VAE loss:4407.34619140625; Total Correlation:3.7647151947021484\n",
      "epoch: 43; iteration: 1408; VAE loss:4401.8671875; Total Correlation:4.176136016845703\n",
      "epoch: 44; iteration: 1440; VAE loss:4395.76708984375; Total Correlation:4.443157196044922\n",
      "epoch: 45; iteration: 1472; VAE loss:4416.64013671875; Total Correlation:3.700490951538086\n",
      "epoch: 46; iteration: 1504; VAE loss:4418.31201171875; Total Correlation:4.568848609924316\n",
      "epoch: 47; iteration: 1536; VAE loss:4383.0302734375; Total Correlation:4.101208686828613\n",
      "epoch: 48; iteration: 1568; VAE loss:4370.36279296875; Total Correlation:4.38099479675293\n",
      "epoch: 49; iteration: 1600; VAE loss:4370.13671875; Total Correlation:3.2565717697143555\n",
      "epoch: 50; iteration: 1632; VAE loss:4427.8525390625; Total Correlation:4.108674049377441\n",
      "epoch: 51; iteration: 1664; VAE loss:4373.97314453125; Total Correlation:2.654743194580078\n",
      "epoch: 52; iteration: 1696; VAE loss:4408.7705078125; Total Correlation:3.7680625915527344\n",
      "epoch: 53; iteration: 1728; VAE loss:4391.46826171875; Total Correlation:4.224658012390137\n",
      "epoch: 54; iteration: 1760; VAE loss:4408.9521484375; Total Correlation:3.915874481201172\n",
      "epoch: 55; iteration: 1792; VAE loss:4454.22705078125; Total Correlation:4.831708908081055\n",
      "epoch: 56; iteration: 1824; VAE loss:4418.2236328125; Total Correlation:4.468864440917969\n",
      "epoch: 57; iteration: 1856; VAE loss:4362.681640625; Total Correlation:3.948847770690918\n",
      "epoch: 58; iteration: 1888; VAE loss:4418.03564453125; Total Correlation:4.795289993286133\n",
      "epoch: 59; iteration: 1920; VAE loss:4417.1494140625; Total Correlation:4.362147331237793\n",
      "epoch: 60; iteration: 1952; VAE loss:4368.2802734375; Total Correlation:3.9104785919189453\n",
      "epoch: 61; iteration: 1984; VAE loss:4397.2841796875; Total Correlation:3.8528308868408203\n",
      "epoch: 62; iteration: 2016; VAE loss:4337.27392578125; Total Correlation:3.16524600982666\n",
      "epoch: 63; iteration: 2048; VAE loss:4393.33544921875; Total Correlation:4.608522415161133\n",
      "epoch: 64; iteration: 2080; VAE loss:4378.2138671875; Total Correlation:4.014373779296875\n",
      "epoch: 65; iteration: 2112; VAE loss:4415.39599609375; Total Correlation:5.1072797775268555\n",
      "epoch: 66; iteration: 2144; VAE loss:4407.1923828125; Total Correlation:4.219283103942871\n",
      "epoch: 67; iteration: 2176; VAE loss:4395.80712890625; Total Correlation:4.249964714050293\n",
      "epoch: 68; iteration: 2208; VAE loss:4393.3134765625; Total Correlation:4.761333465576172\n",
      "epoch: 69; iteration: 2240; VAE loss:4338.73291015625; Total Correlation:4.023355484008789\n",
      "epoch: 70; iteration: 2272; VAE loss:4422.0458984375; Total Correlation:4.520233154296875\n",
      "epoch: 71; iteration: 2304; VAE loss:4405.3623046875; Total Correlation:5.076381683349609\n",
      "epoch: 72; iteration: 2336; VAE loss:4371.56396484375; Total Correlation:4.804705619812012\n",
      "epoch: 73; iteration: 2368; VAE loss:4363.6708984375; Total Correlation:3.2369461059570312\n",
      "epoch: 74; iteration: 2400; VAE loss:4345.36669921875; Total Correlation:3.7405261993408203\n",
      "epoch: 75; iteration: 2432; VAE loss:4346.55029296875; Total Correlation:3.6975460052490234\n",
      "epoch: 76; iteration: 2464; VAE loss:4397.447265625; Total Correlation:4.329520225524902\n",
      "epoch: 77; iteration: 2496; VAE loss:4373.1171875; Total Correlation:4.816239356994629\n",
      "epoch: 78; iteration: 2528; VAE loss:4366.0341796875; Total Correlation:3.710620880126953\n",
      "epoch: 79; iteration: 2560; VAE loss:4409.806640625; Total Correlation:4.733064651489258\n",
      "epoch: 80; iteration: 2592; VAE loss:4383.45556640625; Total Correlation:4.528037071228027\n",
      "epoch: 81; iteration: 2624; VAE loss:4398.07861328125; Total Correlation:4.342126846313477\n",
      "epoch: 82; iteration: 2656; VAE loss:4311.87451171875; Total Correlation:3.1059865951538086\n",
      "epoch: 83; iteration: 2688; VAE loss:4385.64404296875; Total Correlation:5.312477111816406\n",
      "epoch: 84; iteration: 2720; VAE loss:4400.13671875; Total Correlation:3.710660934448242\n",
      "epoch: 85; iteration: 2752; VAE loss:4416.771484375; Total Correlation:4.3288726806640625\n",
      "epoch: 86; iteration: 2784; VAE loss:4344.23583984375; Total Correlation:3.6022520065307617\n",
      "epoch: 87; iteration: 2816; VAE loss:4317.296875; Total Correlation:2.561945915222168\n",
      "epoch: 88; iteration: 2848; VAE loss:4406.248046875; Total Correlation:5.046413421630859\n",
      "epoch: 89; iteration: 2880; VAE loss:4370.4990234375; Total Correlation:4.495019912719727\n",
      "epoch: 90; iteration: 2912; VAE loss:4395.5458984375; Total Correlation:4.601702690124512\n",
      "epoch: 91; iteration: 2944; VAE loss:4390.490234375; Total Correlation:4.127351760864258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92; iteration: 2976; VAE loss:4388.32568359375; Total Correlation:4.484866142272949\n",
      "epoch: 93; iteration: 3008; VAE loss:4383.00244140625; Total Correlation:4.424322128295898\n",
      "epoch: 94; iteration: 3040; VAE loss:4389.6962890625; Total Correlation:4.92341423034668\n",
      "epoch: 95; iteration: 3072; VAE loss:4387.13134765625; Total Correlation:4.834856986999512\n",
      "epoch: 96; iteration: 3104; VAE loss:4349.37353515625; Total Correlation:4.416884422302246\n",
      "epoch: 97; iteration: 3136; VAE loss:4383.08349609375; Total Correlation:4.140979766845703\n",
      "epoch: 98; iteration: 3168; VAE loss:4355.33935546875; Total Correlation:4.348518371582031\n",
      "epoch: 99; iteration: 3200; VAE loss:4362.7841796875; Total Correlation:4.213840484619141\n"
     ]
    }
   ],
   "source": [
    "x_input = data_matrix.copy()\n",
    "index_shuffle = list(range(opt.num_cells_train))\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(opt.n_train_epochs):\n",
    "    # shuffling the data per epoch\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    x_input = x_input[index_shuffle, :]\n",
    "\n",
    "    for i in range(0, opt.num_cells_train // opt.vae_batch_size):\n",
    "\n",
    "        # train VAE/beta-TCVAE in each minibatch\n",
    "        x_data = sample_X(x_input, opt.vae_batch_size)\n",
    "        z_data = noise_prior(opt.vae_batch_size, opt.code_size)\n",
    "        sess.run([opt_vae], {X_v : x_data, z_v: z_data, time_step : current_step})\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "    obj_vae_value, TC_value = sess.run([obj_vae, TotalCorre_mss], {X_v : x_data, z_v: z_data, time_step : current_step})\n",
    "\n",
    "    print('epoch: {}; iteration: {}; VAE loss:{}; Total Correlation:{}'.format(epoch, current_step, obj_vae_value, TC_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = opt.model_path + \"models_vae\"\n",
    "saving_model = saver.save(sess, model_file_path, global_step = current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
