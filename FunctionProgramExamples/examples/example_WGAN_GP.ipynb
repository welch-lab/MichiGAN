{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as ds\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "from nets import *\n",
    "from lib import *\n",
    "\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, num_cells_train, gex_size):\n",
    "        self.num_cells_train =  num_cells_train  # number of cells\n",
    "        self.gex_size = gex_size                 # number of genes\n",
    "        self.epsilon_use = 1e-16                 # small constant value\n",
    "        self.n_train_epochs = 10                 # number of epochs for training\n",
    "        self.batch_size = 32                     # batch size of the GAN-based methods\n",
    "        self.vae_batch_size = 128                # batch size of the VAE-based methods\n",
    "        self.code_size = 10                      # number of codes\n",
    "        self.noise_size = 118                    # number of noise variables\n",
    "        self.inflate_to_size1 = 256              # number of neurons\n",
    "        self.inflate_to_size2 = 512              # number of neurons\n",
    "        self.inflate_to_size3 = 1024             # number of neurons\n",
    "        self.TotalCorrelation_lamb = 100.0         # hyperparameter for the total correlation penalty in beta-TCVAE\n",
    "        self.InfoGAN_fix_std = True              # fixing the standard deviation or not for the Q network of InfoGAN\n",
    "        self.dropout_rate = 0.2                  # dropout hyperparameter\n",
    "        self.disc_internal_size1 = 1024          # number of neurons\n",
    "        self.disc_internal_size2 = 512           # number of neurons\n",
    "        self.disc_internal_size3 = 10            # number of neurons\n",
    "        self.num_cells_generate = 3000           # number of sampled cells\n",
    "        self.GradientPenaly_lambda = 10.0        # hyperparameter for the gradient penalty of Wasserstein GANs\n",
    "        self.latentSample_size = 1               # number of samples of the encoder of VAEs\n",
    "        self.MutualInformation_lamb = 10.0       # hyperparameter for the mutual information penalty in InfoGAN\n",
    "        self.Diters = 5                          # number of training discriminator network per training of generator network of Wasserstein GANs\n",
    "        self.model_path = \"./examples/models_wgangp/\"        # path saving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load data and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.load('./data/TabulaMurisHeart_Processed.npy')\n",
    "data_meta = pd.read_csv(\"./data/TabulaMurisHeart_MetaInformation.csv\")\n",
    "opt = Options(data_matrix.shape[0], data_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Define network tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.placeholder(tf.float32, shape = (None, opt.code_size))\n",
    "X = tf.placeholder(tf.float32, shape = (None, opt.gex_size))\n",
    "\n",
    "## generator\n",
    "X_gen_data = gan_generator(z, opt)\n",
    "\n",
    "\n",
    "## discriminator \n",
    "Dx_real, Dx_hidden_real = gan_discriminator(X, opt)\n",
    "Dx_fake, Dx_hidden_fake = gan_discriminator(X_gen_data, opt, reuse = True)\n",
    "\n",
    "## discriminator loss\n",
    "obj_d_or = tf.reduce_mean(Dx_real) - tf.reduce_mean(Dx_fake) \n",
    "gradient_penalty = compute_gp(X, X_gen_data, gan_discriminator, opt)\n",
    "obj_d = obj_d_or + opt.GradientPenaly_lambda * gradient_penalty\n",
    "\n",
    "## generator loss\n",
    "obj_g = tf.reduce_mean(Dx_fake) \n",
    "\n",
    "## time step\n",
    "time_step = tf.placeholder(tf.int32)\n",
    "\n",
    "## training tensors \n",
    "tf_all_vars = tf.trainable_variables()\n",
    "dvar  = [var for var in tf_all_vars if var.name.startswith(\"Discriminator\")]\n",
    "gvar  = [var for var in tf_all_vars if var.name.startswith(\"Generator\")]\n",
    "\n",
    "\n",
    "### Thanks to taki0112 for the TF StableGAN implementation https://github.com/taki0112/StableGAN-Tensorflow\n",
    "from Adam_prediction import Adam_Prediction_Optimizer\n",
    "opt_g = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = True).minimize(obj_g, var_list = gvar)\n",
    "opt_d = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = False).minimize(obj_d, var_list = dvar)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False, dtype = tf.int32)\n",
    "\n",
    "sess = tf.InteractiveSession()\t\n",
    "init = tf.global_variables_initializer().run()\n",
    "assign_step_zero = tf.assign(global_step, 0)\n",
    "init_step = sess.run(assign_step_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Training the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; iteration: 131; generator loss: -3.311544895172119; discriminator loss:-20.392253875732422\n",
      "epoch: 1; iteration: 262; generator loss: -3.8974978923797607; discriminator loss:-25.575908660888672\n",
      "epoch: 2; iteration: 393; generator loss: -5.372214317321777; discriminator loss:-26.13687515258789\n",
      "epoch: 3; iteration: 524; generator loss: -6.202586650848389; discriminator loss:-27.836223602294922\n",
      "epoch: 4; iteration: 655; generator loss: -7.129977226257324; discriminator loss:-24.760976791381836\n",
      "epoch: 5; iteration: 786; generator loss: -8.601491928100586; discriminator loss:-22.83168601989746\n",
      "epoch: 6; iteration: 917; generator loss: -9.59790325164795; discriminator loss:-28.162975311279297\n",
      "epoch: 7; iteration: 1048; generator loss: -12.712932586669922; discriminator loss:-21.377395629882812\n",
      "epoch: 8; iteration: 1179; generator loss: -13.622909545898438; discriminator loss:-20.044784545898438\n",
      "epoch: 9; iteration: 1310; generator loss: -15.39102840423584; discriminator loss:-19.36362075805664\n"
     ]
    }
   ],
   "source": [
    "x_input = data_matrix.copy()\n",
    "index_shuffle = list(range(opt.num_cells_train))\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(opt.n_train_epochs):\n",
    "    # shuffling the data per epoch\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    x_input = x_input[index_shuffle, :]\n",
    "\n",
    "    for i in range(0, opt.num_cells_train // opt.batch_size):\n",
    "        # train discriminator opt.Diters times per training of generator\n",
    "        for k in range(opt.Diters):\n",
    "\n",
    "            x_data = sample_X(x_input, opt.batch_size)\n",
    "            z_data = noise_prior(opt.batch_size, opt.code_size)\n",
    "\n",
    "            sess.run([opt_d], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "\n",
    "        # train generator\n",
    "        x_data = sample_X(x_input, opt.batch_size)\n",
    "        z_data = noise_prior(opt.batch_size, opt.code_size)\n",
    "        sess.run([opt_g], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "    obj_d_value, obj_g_value = sess.run([obj_d, obj_g], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "    print('epoch: {}; iteration: {}; generator loss: {}; discriminator loss:{}'.format(epoch, current_step, obj_g_value, obj_d_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = opt.model_path + \"models_wgangp\"\n",
    "saving_model = saver.save(sess, model_file_path, global_step = current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
