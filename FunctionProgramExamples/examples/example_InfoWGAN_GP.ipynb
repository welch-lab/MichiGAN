{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for training InfoWGAN-GP on Tabula Muris heart data\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as ds\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "from nets import *\n",
    "from lib import *\n",
    "\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, num_cells_train, gex_size):\n",
    "        self.num_cells_train =  num_cells_train  # number of cells\n",
    "        self.gex_size = gex_size                 # number of genes\n",
    "        self.epsilon_use = 1e-16                 # small constant value\n",
    "        self.n_train_epochs = 10                 # number of epochs for training\n",
    "        self.batch_size = 32                     # batch size of the GAN-based methods\n",
    "        self.vae_batch_size = 128                # batch size of the VAE-based methods\n",
    "        self.code_size = 10                      # number of codes\n",
    "        self.noise_size = 118                    # number of noise variables\n",
    "        self.inflate_to_size1 = 256              # number of neurons\n",
    "        self.inflate_to_size2 = 512              # number of neurons\n",
    "        self.inflate_to_size3 = 1024             # number of neurons\n",
    "        self.TotalCorrelation_lamb = 100.0         # hyperparameter for the total correlation penalty in beta-TCVAE\n",
    "        self.InfoGAN_fix_std = True              # fixing the standard deviation or not for the Q network of InfoGAN\n",
    "        self.dropout_rate = 0.2                  # dropout hyperparameter\n",
    "        self.disc_internal_size1 = 1024          # number of neurons\n",
    "        self.disc_internal_size2 = 512           # number of neurons\n",
    "        self.disc_internal_size3 = 10            # number of neurons\n",
    "        self.num_cells_generate = 3000           # number of sampled cells\n",
    "        self.GradientPenaly_lambda = 10.0        # hyperparameter for the gradient penalty of Wasserstein GANs\n",
    "        self.latentSample_size = 1               # number of samples of the encoder of VAEs\n",
    "        self.MutualInformation_lamb = 10.0       # hyperparameter for the mutual information penalty in InfoGAN\n",
    "        self.Diters = 5                          # number of training discriminator network per training of generator network of Wasserstein GANs\n",
    "        self.model_path = \"./examples/models_infowgangp/\"        # path saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load data and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.load('./data/TabulaMurisHeart_Processed.npy')\n",
    "data_meta = pd.read_csv(\"./data/TabulaMurisHeart_MetaInformation.csv\")\n",
    "opt = Options(data_matrix.shape[0], data_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Define network tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.placeholder(tf.float32, shape = (None, (opt.code_size + opt.noise_size) ))\n",
    "X = tf.placeholder(tf.float32, shape = (None, opt.gex_size))\n",
    "\n",
    "## generator\n",
    "X_gen_data = gan_generator(z, opt)\n",
    "\n",
    "\n",
    "## discriminator \n",
    "Dx_real, Dx_qc_real = infogan_discriminator(X, opt)\n",
    "Dx_fake, Dx_qc_fake = infogan_discriminator(X_gen_data, opt, reuse = True)\n",
    "\n",
    "### compute E(log[q(c|X)])\n",
    "q_vec = MutualInformationLowerBound(Dx_qc_fake, z, opt)\n",
    "q_mutual = tf.reduce_mean(q_vec)\n",
    "\n",
    "## discriminator loss\n",
    "obj_d_or = tf.reduce_mean(Dx_real) - tf.reduce_mean(Dx_fake) \n",
    "gradient_penalty = compute_gp(X, X_gen_data, infogan_discriminator, opt)\n",
    "obj_d_orgp = obj_d_or + opt.GradientPenaly_lambda * gradient_penalty\n",
    "obj_d = obj_d_orgp - (q_mutual * opt.MutualInformation_lamb) \n",
    "\n",
    "## generator loss\n",
    "obj_g = tf.reduce_mean(Dx_fake) - (q_mutual * opt.MutualInformation_lamb) \n",
    "\n",
    "## time step\n",
    "time_step = tf.placeholder(tf.int32)\n",
    "\n",
    "## training tensors \n",
    "tf_all_vars = tf.trainable_variables()\n",
    "dvar  = [var for var in tf_all_vars if var.name.startswith(\"InfoGANDiscriminator\")]\n",
    "gvar  = [var for var in tf_all_vars if var.name.startswith(\"Generator\")]\n",
    "\n",
    "\n",
    "### Thanks to taki0112 for the TF StableGAN implementation https://github.com/taki0112/StableGAN-Tensorflow\n",
    "from Adam_prediction import Adam_Prediction_Optimizer\n",
    "opt_g = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = True).minimize(obj_g, var_list = gvar)\n",
    "opt_d = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = False).minimize(obj_d, var_list = dvar)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False, dtype = tf.int32)\n",
    "\n",
    "sess = tf.InteractiveSession()\t\n",
    "init = tf.global_variables_initializer().run()\n",
    "assign_step_zero = tf.assign(global_step, 0)\n",
    "init_step = sess.run(assign_step_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Training the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; iteration: 131; generator loss: 131.2275390625; discriminator loss:102.63224029541016; mutual information lower bound:-12.827133178710938\n",
      "epoch: 1; iteration: 262; generator loss: 123.05396270751953; discriminator loss:97.433349609375; mutual information lower bound:-12.039224624633789\n",
      "epoch: 2; iteration: 393; generator loss: 110.24713897705078; discriminator loss:86.52019500732422; mutual information lower bound:-11.099255561828613\n",
      "epoch: 3; iteration: 524; generator loss: 112.3279037475586; discriminator loss:83.61398315429688; mutual information lower bound:-11.302135467529297\n",
      "epoch: 4; iteration: 655; generator loss: 115.56700134277344; discriminator loss:83.59393310546875; mutual information lower bound:-11.491128921508789\n",
      "epoch: 5; iteration: 786; generator loss: 108.90390014648438; discriminator loss:78.45838928222656; mutual information lower bound:-10.860621452331543\n",
      "epoch: 6; iteration: 917; generator loss: 110.59671783447266; discriminator loss:84.58761596679688; mutual information lower bound:-11.053926467895508\n",
      "epoch: 7; iteration: 1048; generator loss: 107.9605484008789; discriminator loss:75.30290222167969; mutual information lower bound:-10.788387298583984\n",
      "epoch: 8; iteration: 1179; generator loss: 106.27486419677734; discriminator loss:73.67695617675781; mutual information lower bound:-10.655780792236328\n",
      "epoch: 9; iteration: 1310; generator loss: 104.99896240234375; discriminator loss:68.60572814941406; mutual information lower bound:-10.602582931518555\n"
     ]
    }
   ],
   "source": [
    "x_input = data_matrix.copy()\n",
    "index_shuffle = list(range(opt.num_cells_train))\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(opt.n_train_epochs):\n",
    "    # shuffling the data per epoch\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    x_input = x_input[index_shuffle, :]\n",
    "\n",
    "    for i in range(0, opt.num_cells_train // opt.batch_size):\n",
    "        # train discriminator opt.Diters times per training of generator\n",
    "        for k in range(opt.Diters):\n",
    "\n",
    "            x_data = sample_X(x_input, opt.batch_size)\n",
    "            z_data = noise_prior(opt.batch_size, (opt.code_size + opt.noise_size) )\n",
    "\n",
    "            sess.run([opt_d], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "\n",
    "        # train generator\n",
    "        x_data = sample_X(x_input, opt.batch_size)\n",
    "        z_data = noise_prior(opt.batch_size, (opt.code_size + opt.noise_size) )\n",
    "        sess.run([opt_g], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "    obj_d_value, obj_g_value, obj_q_value = sess.run([obj_d, obj_g, q_mutual], {X : x_data, z: z_data, time_step : current_step})\n",
    "\n",
    "    print('epoch: {}; iteration: {}; generator loss: {}; discriminator loss:{}; mutual information lower bound:{}'.format(epoch, current_step, obj_g_value, obj_d_value, obj_q_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = opt.model_path + \"models_infowgangp\"\n",
    "saving_model = saver.save(sess, model_file_path, global_step = current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
