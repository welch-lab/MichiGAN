{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for training MichiGAN with mean representations from beta-TCVAE (beta = 100) on Tabula Muris heart data\n",
    "\"\"\"\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as ds\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from nets import *\n",
    "from lib import *\n",
    "\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, num_cells_train, gex_size):\n",
    "        self.num_cells_train =  num_cells_train  # number of cells\n",
    "        self.gex_size = gex_size                 # number of genes\n",
    "        self.epsilon_use = 1e-16                 # small constant value\n",
    "        self.n_train_epochs = 10                 # number of epochs for training\n",
    "        self.batch_size = 32                     # batch size of the GAN-based methods\n",
    "        self.vae_batch_size = 128                # batch size of the VAE-based methods\n",
    "        self.code_size = 10                      # number of codes\n",
    "        self.noise_size = 118                    # number of noise variables\n",
    "        self.inflate_to_size1 = 256              # number of neurons\n",
    "        self.inflate_to_size2 = 512              # number of neurons\n",
    "        self.inflate_to_size3 = 1024             # number of neurons\n",
    "        self.TotalCorrelation_lamb = 100.0         # hyperparameter for the total correlation penalty in beta-TCVAE\n",
    "        self.InfoGAN_fix_std = True              # fixing the standard deviation or not for the Q network of InfoGAN\n",
    "        self.dropout_rate = 0.2                  # dropout hyperparameter\n",
    "        self.disc_internal_size1 = 1024          # number of neurons\n",
    "        self.disc_internal_size2 = 512           # number of neurons\n",
    "        self.disc_internal_size3 = 10            # number of neurons\n",
    "        self.num_cells_generate = 3000           # number of sampled cells\n",
    "        self.GradientPenaly_lambda = 10.0        # hyperparameter for the gradient penalty of Wasserstein GANs\n",
    "        self.latentSample_size = 1               # number of samples of the encoder of VAEs\n",
    "        self.MutualInformation_lamb = 10.0       # hyperparameter for the mutual information penalty in InfoGAN\n",
    "        self.Diters = 5                          # number of training discriminator network per training of generator network of Wasserstein GANs\n",
    "        self.model_path = \"./examples/models_michigan_mean/\"        # path saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the beta-TCVAE model\n",
    "path_betatcvae = \"./examples/models_tcvae/models_tcvae-3200\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Load data and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.load('./data/TabulaMurisHeart_Processed.npy')\n",
    "data_meta = pd.read_csv(\"./data/TabulaMurisHeart_MetaInformation.csv\")\n",
    "opt = Options(data_matrix.shape[0], data_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 beta-TCVAE: define network tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_v = tf.placeholder(tf.float32, shape = (None, opt.code_size))\n",
    "X_v = tf.placeholder(tf.float32, shape = (None, opt.gex_size))\n",
    "\n",
    "## encoder\n",
    "z_gen_mean_v, z_gen_std_v = vaes_encoder(X_v, opt)\n",
    "\n",
    "### reparameterization of latent space\n",
    "batch_size = tf.shape(z_gen_mean_v)[0]\n",
    "eps = tf.random_normal(shape=[batch_size, opt.code_size])\n",
    "z_gen_data_v = z_gen_mean_v + z_gen_std_v * eps\n",
    "\n",
    "### latent entropies in a minibatch\n",
    "margin_entropy_mss, joint_entropy_mss = estimate_minibatch_mss_entropy(z_gen_mean_v, z_gen_std_v, z_gen_data_v, opt)\n",
    "### total correlation in a minibatch\n",
    "TotalCorre_mss = tf.reduce_sum(margin_entropy_mss) - tf.reduce_sum(joint_entropy_mss)\n",
    "\n",
    "## decoder\n",
    "z_gen_decoder = vaes_decoder(z_gen_data_v, opt)\n",
    "\n",
    "### generated data\n",
    "X_gen_data = z_gen_decoder.sample(opt.latentSample_size)\n",
    "X_gen_data = tf.reshape(X_gen_data , tf.shape(X_gen_data)[1:])\n",
    "\n",
    "## loss elements\n",
    "### reconstruction error\n",
    "z_gen_de = z_gen_decoder.log_prob(X_v)\n",
    "z_gen_de_value = tf.reduce_sum(z_gen_de, [1])\n",
    "rec_x_loss = - tf.reduce_mean(z_gen_de_value)\n",
    "\n",
    "### latent prior and posterior probabilities\n",
    "stg_prior = tf_standardGaussian_prior(tf.shape(X_v)[0], opt.code_size)\n",
    "latent_prior = stg_prior.log_prob(z_gen_data_v)\n",
    "latent_posterior = c_mutual_mu_var_entropy(z_gen_mean_v, z_gen_std_v, z_gen_data_v, opt)\n",
    "\n",
    "### latent joint prior and posterior probabilities\n",
    "latent_prior_joint = tf.reduce_sum(latent_prior, [1])\n",
    "latent_posterior_joint = tf.reduce_sum(latent_posterior, [1])\n",
    "\n",
    "### KL divergence\n",
    "kl_latent = - tf.reduce_mean(latent_prior_joint) + tf.reduce_mean(latent_posterior_joint)\n",
    "\n",
    "### VAE/beta-TCVAE loss function\n",
    "obj_vae = rec_x_loss  + kl_latent + opt.TotalCorrelation_lamb * TotalCorre_mss\n",
    "\n",
    "## time step\n",
    "time_step = tf.placeholder(tf.int32)\n",
    "\n",
    "## training tensors \n",
    "tf_all_vars = tf.trainable_variables()\n",
    "encodervar  = [var for var in tf_all_vars if var.name.startswith(\"EncoderX2Z\")]\n",
    "decodervar  = [var for var in tf_all_vars if var.name.startswith(\"DecoderZ2X\")]\n",
    "\n",
    "optimizer_vae = tf.train.AdamOptimizer(1e-4)\n",
    "opt_vae = optimizer_vae.minimize(obj_vae, var_list = encodervar + decodervar)\n",
    "\n",
    "saver1 = tf.train.Saver()\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Load the trained beta-TCVAE model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess1 = tf.InteractiveSession()\t\n",
    "init = tf.global_variables_initializer().run()\n",
    "saver1.restore(sess1,  path_betatcvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 MichiGAN: define network tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.placeholder(tf.float32, shape = (None, opt.noise_size))\n",
    "X = tf.placeholder(tf.float32, shape = (None, opt.gex_size))\n",
    "Y = tf.placeholder(tf.float32, shape = (None, opt.code_size))\n",
    "\n",
    "## generator\n",
    "X_gen_data = michigan_generator(z, Y, opt)\n",
    "\n",
    "## discriminator\n",
    "Dx_real, Dx_hidden_real = michigan_discriminator(X, Y, opt)\n",
    "Dx_fake, Dx_hidden_fake = michigan_discriminator(X_gen_data, Y, opt, reuse = True)\n",
    "\n",
    "\n",
    "## discriminator loss\n",
    "obj_d_or = tf.reduce_mean(Dx_real) - tf.reduce_mean(Dx_fake) \n",
    "gradient_penalty = compute_con_gp(X, X_gen_data, Y, michigan_discriminator, opt)\n",
    "obj_d = obj_d_or + opt.GradientPenaly_lambda * gradient_penalty\n",
    "\n",
    "## generator loss\n",
    "obj_g = tf.reduce_mean(Dx_fake)\n",
    "\n",
    "\n",
    "## time step\n",
    "time_step = tf.placeholder(tf.int32)\n",
    "\n",
    "\n",
    "## training tensors \n",
    "tf_all_vars = tf.trainable_variables()\n",
    "dvar  = [var for var in tf_all_vars if var.name.startswith(\"MichiGANDiscriminator\")]\n",
    "gvar  = [var for var in tf_all_vars if var.name.startswith(\"MichiGANGenerator\")]\n",
    "\n",
    "\n",
    "### Thanks to taki0112 for the TF StableGAN implementation https://github.com/taki0112/StableGAN-Tensorflow\n",
    "from Adam_prediction import Adam_Prediction_Optimizer\n",
    "opt_g = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = True).minimize(obj_g, var_list = gvar)\n",
    "opt_d = Adam_Prediction_Optimizer(learning_rate = 1e-4, beta1 = 0.9, beta2 = 0.999, prediction = False).minimize(obj_d, var_list = dvar)\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False, dtype = tf.int32)\n",
    "\n",
    "sess = tf.InteractiveSession()\t\n",
    "init = tf.global_variables_initializer().run()\n",
    "assign_step_zero = tf.assign(global_step, 0)\n",
    "init_step = sess.run(assign_step_zero)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Training the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; iteration: 131; generator loss: -2.6590664386749268; discriminator loss:-11.31005573272705\n",
      "epoch: 1; iteration: 262; generator loss: -3.035510540008545; discriminator loss:-12.085517883300781\n",
      "epoch: 2; iteration: 393; generator loss: -4.136909484863281; discriminator loss:-17.024450302124023\n",
      "epoch: 3; iteration: 524; generator loss: -4.078310012817383; discriminator loss:-20.188270568847656\n",
      "epoch: 4; iteration: 655; generator loss: -3.7561745643615723; discriminator loss:-21.325881958007812\n",
      "epoch: 5; iteration: 786; generator loss: -2.2448720932006836; discriminator loss:-24.080490112304688\n",
      "epoch: 6; iteration: 917; generator loss: -1.8978437185287476; discriminator loss:-24.484867095947266\n",
      "epoch: 7; iteration: 1048; generator loss: -1.6046117544174194; discriminator loss:-23.0334415435791\n",
      "epoch: 8; iteration: 1179; generator loss: 0.51566082239151; discriminator loss:-28.99704360961914\n",
      "epoch: 9; iteration: 1310; generator loss: -2.3488593101501465; discriminator loss:-23.731531143188477\n"
     ]
    }
   ],
   "source": [
    "x_input = data_matrix.copy()\n",
    "index_shuffle = list(range(opt.num_cells_train))\n",
    "current_step = 0\n",
    "\n",
    "## beta-TCVAE mean representations \n",
    "representations_mean = sess1.run(z_gen_mean_v, {X_v: data_matrix})\n",
    "y_input = representations_mean.copy()\n",
    "\n",
    "for epoch in range(opt.n_train_epochs):\n",
    "    # shuffling the data per epoch\n",
    "    np.random.shuffle(index_shuffle)\n",
    "    x_input = x_input[index_shuffle, :]\n",
    "    y_input = y_input[index_shuffle, :]\n",
    "\n",
    "    for i in range(0, opt.num_cells_train // opt.batch_size):\n",
    "        # train discriminator opt.Diters times per training of generator\n",
    "        for k in range(opt.Diters):\n",
    "\n",
    "            x_data, y_data = sample_XY(x_input, y_input, opt.batch_size)\n",
    "            z_data = noise_prior(opt.batch_size, opt.noise_size)\n",
    "\n",
    "            sess.run([opt_d], {X : x_data, Y: y_data, z: z_data, time_step : current_step})\n",
    "\n",
    "\n",
    "        # train generator\n",
    "        x_data, y_data = sample_XY(x_input, y_input, opt.batch_size)\n",
    "        z_data = noise_prior(opt.batch_size, opt.noise_size)\n",
    "        sess.run([opt_g], {X : x_data, Y: y_data, z: z_data, time_step : current_step})\n",
    "\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "    obj_d_value, obj_g_value = sess.run([obj_d, obj_g], {X : x_data, Y: y_data, z: z_data, time_step : current_step})\n",
    "\n",
    "    print('epoch: {}; iteration: {}; generator loss: {}; discriminator loss:{}'.format(epoch, current_step, obj_g_value, obj_d_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = opt.model_path + \"models_michigan_mean\"\n",
    "saving_model = saver.save(sess, model_file_path, global_step = current_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
